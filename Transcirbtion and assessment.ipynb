{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e4ed92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, math, json, tempfile\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio, display, Markdown\n",
    "import ipywidgets as widgets\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "035fba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path: str, sr: int = 16000) -> Tuple[np.ndarray, int]:\n",
    "    wav, orig_sr = librosa.load(path, sr=sr, mono=True)\n",
    "    return wav, sr\n",
    "\n",
    "# simple token overlap\n",
    "def token_overlap_ratio(a: str, b: str) -> float:\n",
    "    ta = set([w.lower().strip('.,!?;:()[]') for w in a.split() if w.strip()])\n",
    "    tb = set([w.lower().strip('.,!?;:()[]') for w in b.split() if w.strip()])\n",
    "    if not ta and not tb:\n",
    "        return 1.0\n",
    "    if not ta or not tb:\n",
    "        return 0.0\n",
    "    overlap = ta.intersection(tb)\n",
    "    return len(overlap) / max(1, len(ta.union(tb)))\n",
    "\n",
    "# Simple readability / length metrics\n",
    "def simple_quality_metrics(transcript: str, audio_seconds: float) -> Dict:\n",
    "    words = transcript.split()\n",
    "    wps = len(words) / max(1e-6, audio_seconds)\n",
    "    avg_word_len = sum(len(w) for w in words) / max(1, len(words))\n",
    "    return {'num_words': len(words), 'words_per_sec': wps, 'avg_word_length': avg_word_len}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bdca8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ASR pipeline (this will download model weights the first time)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR pipeline ready.\n",
      "Loading sentence-transformers embedding model...\n",
      "Embedding model ready.\n"
     ]
    }
   ],
   "source": [
    "ASR_MODEL = \"facebook/wav2vec2-large-960h\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "print('Loading ASR pipeline (this will download model weights the first time)...')\n",
    "asr = pipeline('automatic-speech-recognition', model=ASR_MODEL, chunk_length_s=30)\n",
    "print('ASR pipeline ready.')\n",
    "\n",
    "print('Loading sentence-transformers embedding model...')\n",
    "embedder = SentenceTransformer(EMBEDDING_MODEL)\n",
    "print('Embedding model ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7044f8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5536ee59ff94e78ba655818e0b427d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Upload a .wav voice note of the interview answer:'), FileUpload(value=(), accept='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "upload = widgets.FileUpload(accept='.wav', multiple=False)\n",
    "question_text = widgets.Textarea(value='', placeholder='Paste the interview question here (e.g. What is your name?)', description='Question:', layout=widgets.Layout(width='100%', height='80px'))\n",
    "topk_widget = widgets.IntSlider(value=5, min=1, max=15, step=1, description='Top keywords:')\n",
    "run_button = widgets.Button(description='Transcribe & Assess', button_style='primary')\n",
    "\n",
    "out = widgets.Output(layout={'border': '1px solid black'})\n",
    "\n",
    "def on_run_clicked(b):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        if not upload.value:\n",
    "            print('Please upload a .wav file using the upload widget above.')\n",
    "            return\n",
    "        files = upload.value\n",
    "        if isinstance(files, dict):\n",
    "            files = list(files.values())\n",
    "        elif isinstance(files, tuple):\n",
    "            files = list(files)\n",
    "\n",
    "        if not files:\n",
    "            print(\"Please upload a .wav file first.\")\n",
    "            return\n",
    "\n",
    "        fileinfo = files[0]\n",
    "        data = fileinfo['content']\n",
    "        name = fileinfo.get('name', 'uploaded.wav')\n",
    "        tmp_dir = tempfile.gettempdir()\n",
    "        tmp_path = os.path.join(tmp_dir, name)\n",
    "\n",
    "        with open(tmp_path, 'wb') as f:\n",
    "            f.write(data)\n",
    "\n",
    "        print(f\"Saved uploaded file to {tmp_path}\")\n",
    "        wav, sr = load_audio(tmp_path, sr = 16000)\n",
    "        audio_seconds = len(wav) / sr\n",
    "        display(Audio(data=wav, rate=sr))\n",
    "        print('\\nTranscribing audio (this can take a few seconds)...')\n",
    "        wav, sr = librosa.load(tmp_path, sr = 16000, mono = True)\n",
    "\n",
    "        result = asr({\"array\": wav, \"sampling_rate\": sr})\n",
    "        transcript = result['text'].strip()\n",
    "        print('\\n--- TRANSCRIPT ---\\n')\n",
    "        print(transcript if transcript else '[empty transcript]')\n",
    "        print('\\n--- METRICS ---\\n')\n",
    "        q_text = question_text.value.strip()\n",
    "        if not q_text:\n",
    "            print('No question text provided. Please paste the interview question into the Question field.')\n",
    "            return\n",
    "        embeddings = embedder.encode([q_text, transcript], convert_to_numpy=True)\n",
    "        cos = float(cosine_similarity([embeddings[0]], [embeddings[1]])[0][0])\n",
    "        overlap = token_overlap_ratio(q_text, transcript)\n",
    "        quality = simple_quality_metrics(transcript, audio_seconds)\n",
    "        # Weighted combination: 70% semantic (cos), 20% token overlap, 10% length/quality sanity check\n",
    "        length_score = min(1.0, quality['num_words'] / max(1, len(q_text.split())))  # if answer length similar to question length -> ok\n",
    "        raw_score = 0.7 * cos + 0.2 * overlap + 0.1 * length_score\n",
    "        score_0_100 = round(float(raw_score * 100), 2)\n",
    "        print(f'Cosine similarity (semantic): {cos:.4f}')\n",
    "        print(f'Token overlap ratio: {overlap:.4f}')\n",
    "        print(f'Answer words: {quality['num_words']}, words/sec: {quality['words_per_sec']:.2f}')\n",
    "        print(f'Length-based score: {length_score:.4f}')\n",
    "        print(f'--> Aggregate relevance score (0-100): {score_0_100}')\n",
    "        # Save results\n",
    "        out_json = {\n",
    "            'question': q_text,\n",
    "            'transcript': transcript,\n",
    "            'cosine_similarity': cos,\n",
    "            'token_overlap': overlap,\n",
    "            'quality': quality,\n",
    "            'aggregate_relevance_score': score_0_100\n",
    "        }\n",
    "        tmp_dir = tempfile.gettempdir()\n",
    "        out_path = os.path.join(tmp_dir, \"transcription_relevance_result.json\")\n",
    "\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as fh:\n",
    "            json.dump(out_json, fh, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(\"Full results saved to:\", out_path)\n",
    "        \n",
    "run_button.on_click(on_run_clicked)\n",
    "\n",
    "display(widgets.VBox([widgets.Label('Upload a .wav voice note of the interview answer:'), upload, question_text, run_button, out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432e819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def assess_file(path: str, question: str) -> Dict:\n",
    "    wav, sr = load_audio(path, sr=16000)\n",
    "    audio_seconds = len(wav) / sr\n",
    "    print(f'Audio length: {audio_seconds:.2f}s — running ASR...')\n",
    "    res = asr(path)\n",
    "    transcript = res.get('text', '').strip()\n",
    "    print('Transcript:', transcript)\n",
    "    emb = embedder.encode([question, transcript], convert_to_numpy=True)\n",
    "    cos = float(cosine_similarity([emb[0]], [emb[1]])[0][0])\n",
    "    overlap = token_overlap_ratio(question, transcript)\n",
    "    quality = simple_quality_metrics(transcript, audio_seconds)\n",
    "    length_score = min(1.0, quality['num_words'] / max(1, len(question.split())))\n",
    "    raw_score = 0.7 * cos + 0.2 * overlap + 0.1 * length_score\n",
    "    score_0_100 = round(float(raw_score * 100), 2)\n",
    "    return {\n",
    "        'question': question,\n",
    "        'transcript': transcript,\n",
    "        'cosine_similarity': cos,\n",
    "        'token_overlap': overlap,\n",
    "        'quality': quality,\n",
    "        'aggregate_relevance_score': score_0_100\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
